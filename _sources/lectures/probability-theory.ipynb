{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Probability theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Note that this lecture is currently incomplete; if you come back to the page and this message is gone, you can consider it complete.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Recap of last time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In the [Introductory lecture](./intro), we X\n",
    "\n",
    "what we'll covere here: **probability theory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Today: Probability theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Before jumping in further, let's re-load that same dataset, and while we're at it import the needed packages we'll need to make nice plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "filepath_in = \"../data/central-park-station-data.nc\"\n",
    "ds_central_park = xr.open_dataset(filepath_in)\n",
    "precip_central_park = ds_central_park[\"precip\"]\n",
    "temp_central_park = ds_central_park[\"temp_avg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# First, import the matplotlib package that we'll use for plotting.\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Then update the plotting aesthetics using my own custom package named \"puffins\"\n",
    "# See: https://github.com/spencerahill/puffins\n",
    "from puffins import plotting as pplt\n",
    "plt.rcParams.update(pplt.plt_rc_params_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 3 axioms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the 3 axioms:\n",
    "\n",
    "1. **Non-negativity**: the probability $P$ of any event $E$ is at least zero: $P(E)\\geq0$\n",
    "2. **X**: the probability $P$ of the sample space $S$ is unity: $P(S)=1$.\n",
    "3. **Additivity**: For two mutually exclusive events $E_1$ and $E_2$, the probability of their union $E_1\\cup E_2$ is the sum of their individual probabilities: $P(E_1\\cup E_2)=P(E_1)+P(E_2)$\n",
    "\n",
    "In class, we used these to formally prove a couple things:\n",
    "\n",
    "- All probabilities are bounded by 0 and 1: for any event $E$, $0\\leq P(E) \\leq 1$.\n",
    "- Probability of the **complement**: If $E$ is an event and $E^C$ is its complement, then $P(E^C)=1-P(E)$\n",
    "\n",
    "Use these to prove the following: $P((E_1\\cup E_2)^C)=1-P(E_1\\cup E_2)=1-[P(E_1)+P(E_2)-P(E_1\\cap E_2)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations, random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider each **observation**---that is, each inidividual value in any dataset, to be the result of an experiment performed on nature.\n",
    "\n",
    "A **random variable** is a function that maps the outcome of any observation to a real number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete vs. continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **discrete** variable is one that can only take on a finite number of values.  A **continuous** variable is one that can take on infinitely many values.\n",
    "\n",
    "In practice, often physical quantities that are in reality continuous, like temperature, end up as effectively discrete, because they are only reported up to a finite precision.  But unless this precision is quite coarse, we can usually still usefully treat them as if they really were continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability mass and density functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For discrete random variables, the probability mass function specifies the probability of every possible outcome of that variable.  For example, the probability mass function of a fair 6-sided dice would be 1/6 for each of the 6 faces, since they're all equally likely.  \n",
    "\n",
    "Notice in this dice roll case that the probability mass function summed over all possible up to exactly one...that is true for all probability mass functions.\n",
    "\n",
    "For *continuous* random variables, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative distribution functions\n",
    "\n",
    "The **cumulative distribution function** (CDF) of a random variable---whether continuous or discrete---gives the probability for each possible value that the variable is less than or equal to that value.  In other words, for each value $x$, it gives the corresponding **quantile**.  As such, it always ranges from 0 (for values less than the variable's minimum value, or for $-\\infty$ if there is no minimum value) to 1 (for values greater than the variable's maximum value, or for $+\\infty$ if there is no maximum value).\n",
    "\n",
    "For discrete variables, the CDF is the sum of the probability mass function over all values less than or equal to the given value: \n",
    "$$F(x_j)=\\sum_{i=1}^j p(x_i),$$\n",
    "where $x_j$ is the value of interest, $f(x)$ is the probability mass function, and the values of $x$ are assumed to be ordered from the smallest value $x_0$ to their largest value $x_N$.\n",
    "\n",
    "For continuous variables, the CDF is the *integral* of the probability density function: \n",
    "$$F(x)=P(X\\leq x)=\\int_{-\\infty}^xp(u)\\,\\mathrm{d}u.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation and population mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, **expectation** (also known as \"expected value\") is simply a probability-weighted average over a random variable.  \n",
    "\n",
    "For a discrete variable, this is\n",
    "$$E[g(X)]=\\sum_{i=1}^Ng(X_i)p_i,$$\n",
    "where $g(X)$ is some function.\n",
    "\n",
    "For a continuous variable, the expectation is\n",
    "$$E[g(X)]=\\int_{-\\infty}^{\\infty}g(x)p(x)\\,\\mathrm{d}x.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal (\"Gaussian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **normal distribution** is crucially important.  Its probability density is given by\n",
    "$$p(x)=\\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right),$$\n",
    "where\n",
    "- $\\mu$ is the mean\n",
    "- $\\sigma$ is the standard deviation\n",
    "\n",
    "If $\\mu=0$ and $\\sigma=1$, the resulting distribution is called the **standard normal**:\n",
    "$$p(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually / in essence: the sum of random variables tends to be Gaussian, whether or not the variable themselves are Gaussian.\n",
    "\n",
    "Formally:\n",
    "\n",
    "Let $X_1$, ..., $X_N$ be independent and identically distributed (\"IID\") random variables, all with identical mean $\\mu_X$ and identical (finite) variance $\\sigma_X$.  *Note that, while they must be IID, their distribution does **not** have to be the Gaussian.*  Then the random variable\n",
    "$$Z=\\frac{\\hat\\mu_X-\\mu_X}{\\sigma_X/\\sqrt{N}}$$\n",
    "converges to the standard normal distribution as $N\\rightarrow\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Supplementary Materials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
